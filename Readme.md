# Magento deployment on AWS using ECS/Fargate, RDS, OpenSearch, and EFS deployed with CDK


The goal of this project is to deploy ECS service with Autoscaling, and relying on a ECS cluster using Capacity Providers with an AutoScaling Group.

## The project is Bootstrap with projen

### Projen installation

```bash
npm install projen
```

#### how to init a project with projen (just for information):

```bash
$ git init
$ npx projen new awscdk-app-ts
```

This creates a `.projenrc.js` file

you can regenerate with

```bash
npx projen
```

I personally create an alias to launch projen
```bash
alias pj='npx projen'
```

```bash
pj build
```


### Install project dependencies.

The dependencies cdk or npm packages must be installed through projen (in the `.projenrx.js` file)

exemple:

```json
  cdkDependencies: [
    '@aws-cdk/aws-certificatemanager',
    '@aws-cdk/aws-ec2',
    '@aws-cdk/aws-ecr',
```
then execute pj to install thems

```bash
pj
```


### Project Configuration

First of all, you must specify a stack name that would be used to create your stack. This is done with an environment parameter `CDK_STACK_NAME` wich default to `magento`.

you can export it in your environment before creating the stack.
Others ressources, can be also created based on the stack name, but you can also control this using the cdk Context parameters used through projen, see below:

In the `.projenrc.js` file you can configure how the stack will be deployed inside your AWS Account.

After updating the **context** section you will need to run again `pj` in order to generate the appropriate cdk.json file from the projen bootstrap structure.

```json
...
  context: {
    vpc_tag_name: 'ecsworkshop-base/BaseVPC', // TAG Name of the VPC to create the cluster into (or 'default' or remove to create new one)
    enablePrivateLink: 'true',

    //os_domain: 'magento-cdk', // default to $CDK_STACK_NAME
    os_master_user_name: 'magento-master-os',

    //db_name: 'magento', // default to env $CDK_STACK_NAME
    db_user: 'magentodbuser',

    route53_domain_zone: 'ecs.demo3.allamand.com',
    //route53_magento_prefix: 'magento', // default to $CDK_STACK_NAME
    //route53_eksutils_prefix: 'eksutils', // default to $CDK_STACK_NAME-eksutils

    magento_user: 'user1',
    magento_debug_task: 'yes',
  },
  ...
```

- **vpc_tag_name** : You can specify a tag name to use existing VPC, or ommit this parameter to create new VPC from CDK
- **enablePrivateLink** if true, enable VPC service endpoints for Cloudwatch, EFS, SecretManager. (@default: no)

OpenSearch cluster parameters (password is generated by CDK and stored in SecretManager with name "$CDK_STACK_NAME-magento-opensearch-admin-password"):

- **os_domain** : the name of the OpenSearch Cluster that the cdk stack will create for you (@default: <stack_name>)
- **os_master_user_name**: the name for the master
- TODO: make rise of os_master_user_password

RDS Database (password is generated by CDK and stored in SecretManager with name "$CDK_STACK_NAME-magento"):

- **db_name**: the name of the db to create (@default: $CDK_STACK_NAME)
- **db_user**: the name of the db user (@default: magentouser)

ECS Cluster

- **route53_domain_zone** (MANDATORY) needs to specify the Route53 zone you want to uses to deploy your services
- **route53_magento_prefix**: preffix to uses for exposing magento service (@default: $CDK_STACK_NAME)
- **route53_eksutils_prefix**: prefix to uses for exposing the eksutils service (@default: $CDK_STACK_NAME-eksutils )

> **You need to have prior to this created a wildcard certificate for your route53 zone in Certificate Manager and store this arn in Parameter store with key `CertificateArn-<route53_domain_zone>`**

Magento (password is generated by CDK and stored in SecretManager with name "<stackname>-magento-database-password"):)

- **magento_user**: magento user (@default: magento)
- **magento_debug_task**: yes/no - do we start another magento service with just bash (not running magento instance) for debugging or executing scripts (@default no)

## Generate and deploy

Synthetize and generates the CloudFormation template from the cdk Typescript code:

```bash
make synth
```

Deploy the stack into your AWS account

```bash
make deploy
```


## TroubleShoot and debug

### Connect to the Magento Debug Task

If you activate the context parameter `magento_debug_task` then the Stack creates a dedicated task that is deployed with all same parameters than the magento task, except, that it don't start magento, and don't get exposed through a load balancer. But it is link with same, DB, same Opensearch and same Elastic Fils system.

You can use this task to interract with your magento setup, or debug things.

### Troubleshoot first install

When Magento starts, it will execute the following command:

```bash
/opt/bitnami/scripts/magento/entrypoint.sh /opt/bitnami/scripts/magento/run.sh
```

If the Task didn't start properly, you can exec into the debug pod and manually execute the previous command to help figure out where comes the problem. This can be credentials issues, passwords format like for example :

```
In SearchConfig.php line 81:
                                                                               
  Could not validate a connection to Elasticsearch. Could not parse URI: "htt  
  ps://magento-master-os:beavqpdh.Kzm4?6WqtJHv4e0Lj3AioyI@search-magento-zwa5  
  v3x4br3kgn4y5e5nu6hv7q.eu-west-1.es.amazonaws.com:443"                       
                                                                           
```   

### Install magento demo content

If you want to install specific magento content, you can uses the magentoDebug service to connect into.
Use the Cdk output (or CloudFormation output in console) to get the connect command:exemple : `ecs_exec_service magento2 magento2-MagentoServiceDebugmagentoServiceMagentoServiceDebugService7B086C58-Danj7j20EkiI magento`

This will enable a secure shell (The session is encrypted with a dedicated AWS KMS key generated for you by cdk) within your tasks, and all your commands will be stored in the S3 bucket

Example commands to install magento sample datas (https://github.com/magento/magento2-sample-data):

```bash
cd /bitnai/magento
composer update

php -d memory_limit=-1 bin/magento sampledata:deploy
```

#php -d memory_limit=-1 /opt/bitnami/magento/bin/magento setup:upgrade

#php /opt/bitnami/magento/bin/magento setup:static-content:deploy -f


composer diagnose
composer self-update

https://github.com/magento/magento2-sample-data
vi bitnami/magento/composer.json

        "magento/module-catalog-sample-data": "100.4.*",
        "magento/module-configurable-sample-data": "100.4.*",
        "magento/module-cms-sample-data": "100.4.*",
        "magento/module-sales-sample-data": "100.4.*"

### Set developper mode

php /opt/bitnami/magento/bin/magento deploy:mode:set developer


# Troubleshoot magento

uses the script to exec into the task

```
ecs_exec_service magento magento-magentoService27FB24D3-VIUJrhZwrS2S magento
```

debug commands

```
apt-get update && apt-get install -y vim
set -o xtrace
/opt/bitnami/scripts/magento/setup.sh | less
source /opt/bitnami/scripts/magento/setup.sh  | more
```

magento execute this script at startup : `/bin/bash /opt/bitnami/scripts/magento/setup.sh`

## Mysql

ensure_dir_exists /bitnami/magento
configure_permissions_ownership /bitnami/magento -d 775 -f 664 -u daemon -g root
magento_wait_for_db_connection $MAGENTO_DATABASE_HOST 3306 magento magentouser 'Passw0rd!'

mysql -h $MAGENTO_DATABASE_HOST -u $MAGENTO_DATABASE_USER -p$MAGENTO_DATABASE_PASSWORD $MAGENTO_DATABASE_NAME

## Elasticsearch

You can test the elasticsearch connection in curl with

```
curl -XPOST -u "$MAGENTO_ELASTICSEARCH_USER:$MAGENTO_ELASTICSEARCH_PASSWORD" "https://$ELASTICSEARCH_HOST/_search" -H "content-type:application/json" -d'
{
"query": {
"match_all": {}
}
}'
```

### Deleting the Stack

The stack is configured to delete the database cluster and openshift cluster, and EFS file system. If you want to be able to keep the datas, you will need to update the removalPolicy policies of thoses service in the cdk creation code.

```typescript
    const db = new DatabaseCluster(this, 'ServerlessWordpressAuroraCluster', {
      engine: DatabaseClusterEngine.AURORA_MYSQL,
      credentials: Credentials.fromPassword(DB_USER, secret),
      removalPolicy: RemovalPolicy.DESTROY, // <-- you can change this -->
      instanceProps: {
        vpc: vpc,
        securityGroups: [rdsSG],
      },
      defaultDatabaseName: DB_NAME,
    });
    ...

    const osDomain = new opensearch.Domain(this, 'Domain', {
      version: opensearch.EngineVersion.OPENSEARCH_1_0,
      domainName: OS_DOMAIN,
      //accessPolicies: [osPolicy], // Default No access policies
      removalPolicy: RemovalPolicy.DESTROY, // <-- you can change this -->
      securityGroups: [openSearchSG],
    ...

    const efsFileSystem = new FileSystem(this, 'FileSystem', {
      vpc,
      securityGroup: efsFileSystemSecurityGroup,
      performanceMode: PerformanceMode.GENERAL_PURPOSE,
      lifecyclePolicy: LifecyclePolicy.AFTER_30_DAYS,
      throughputMode: ThroughputMode.BURSTING,
      encrypted: true,
      removalPolicy: RemovalPolicy.DESTROY,// <-- you can change this -->
    });
```    


While we can't delete an ECS Capacity Provider when associated Autoscaling Group still exists, the first attempt to delete the stack may finished in a `DELETE_FAILED` state. A second delete atempt should properly delete everything.